Rm_pre2000 <- as.numeric(as.character(prior))
df$Rm <- as.numeric(as.character(df$Rm))   # safe conversion if factor/char
# check
is.numeric(df$Rm)
# Step 2: subset pre-2000 data
Rm_pre2000 <- df$Rm[df$Year <= 2000]
# Step 3: subset post-2000 data
Rm_post2000 <- df$Rm[df$Year > 2000]
h <- (hist(Rm_pre2000, breaks = 10, plot = FALSE))
prior_values <- h$mids
prior_probs  <- h$density / sum(h$density)
prior <- cbind(prior_values, prior_probs)
mode(prior) <- "numeric"
# Likelihood from post-2000 data
mean_post <- mean(Rm_post2000)
sd_post   <- sd(Rm_post2000)
likelihood <- dnorm(prior_values, mean = mean_post, sd = sd_post)
posterior <- discrete.bayes(prior, likelihood)
Rm_post2000 <- df$Rm[df$Year > 2000]
posterior <- discrete.bayes(prior, likelihood)
mean_post <- mean(Rm_post2000, na.rm = TRUE)
sd_post   <- sd(Rm_post2000), na.rm=TRUE)
sd_post   <- sd(Rm_post2000),na.rm=TRUE)
mean_post <- mean(Rm_post2000,na.rm = TRUE)
sd_post   <- sd(Rm_post2000,na.rm=TRUE)
likelihood <- dnorm(prior_values, mean = mean_post, sd = sd_post)
posterior <- discrete.bayes(prior, likelihood)
prior <- as.matrix(prior)
mode(prior) <- "numeric"
# Likelihood from post-2000 data
mean_post <- mean(Rm_post2000,na.rm = TRUE)
sd_post   <- sd(Rm_post2000,na.rm=TRUE)
likelihood <- dnorm(prior_values, mean = mean_post, sd = sd_post)
posterior <- discrete.bayes(prior, likelihood)
str(prior)
mean_post <- mean(Rm_post2000,na.rm = TRUE)
sd_post   <- sd(Rm_post2000,na.rm=TRUE)
likelihood <- dnorm(prior_values, mean = mean_post, sd = sd_post)
# Posterior
posterior <- discrete.bayes(prior, likelihood)
str(prior)
head(prior)
prior <- cbind(as.numeric(prior_values), as.numeric(prior_probs))
prior <- as.matrix(prior)
mode(prior) <- "numeric"
str(prior)
head(prior)
# Likelihood from post-2000 data
mean_post <- mean(Rm_post2000,na.rm = TRUE)
sd_post   <- sd(Rm_post2000,na.rm=TRUE)
likelihood <- dnorm(prior_values, mean = mean_post, sd = sd_post)
posterior <- discrete.bayes(prior, likelihood)
colnames(prior) <- NULL   # drop column names
rownames(prior) <- NULL
mode(prior) <- "numeric"
str(prior)
head(prior)
# Likelihood from post-2000 data
mean_post <- mean(Rm_post2000,na.rm = TRUE)
sd_post   <- sd(Rm_post2000,na.rm=TRUE)
likelihood <- dnorm(prior_values, mean = mean_post, sd = sd_post)
# Posterior
posterior <- discrete.bayes(prior, likelihood)
likelihood <- dnorm(prior_values, mean = mean_post, sd = sd_post)
mean_post <- mean(Rm_post2000,na.rm = TRUE)
sd_post   <- sd(Rm_post2000,na.rm=TRUE)
posterior <- discrete.bayes(prior, likelihood)
E = seq(1,100000)
n = length(E)
E1 = sample(E,length(E)*0.1)
E0 = setdiff (E,E1)
E1I1 = sample(E1, length(E1)*0.6)
E1I0 = setdiff (E1, E1I1)
E0I1 = sample(E0, length(E0)*0.2)
E0I0 = setdiff (E0, E0I1)
E1I1F1 = sample(E1I1 , length(E1I1)*0.95)
E1I1F0 = setdiff (E1I1 , E1I1F1)
E1I0F1 = sample(E1I0 , length(E1I0)*0.70)
E1I0F0 = setdiff (E1I0 , E1I0F1)
E0I1F1 = sample(E0I1 , length(E0I1)*0.80)
E0I1F0 = setdiff (E0I1 , E0I1F1)
E0I0F1 = sample(E0I0 , length(E0I0)*0.10)
E0I0F0 = setdiff (E0I0 , E0I0F1)
pr_I1_given_F1 = length(c(E1I1F1 , E0I1F1 )) /
length(c(E1I1F1 , E1I0F1 , E0I1F1 , E0I0F1 ))
print (pr_I1_given_F1)
x<-c("He said"that's amazing!", "\a\b\c\d'", "\\\\\\\"")
x
F<-c("He said"that's amazing!", "\a\b\c\d'", "\\\\\\\"")
F<-c('He said"that's amazing!'', "\a\b\c\d'", "\\\\\\\"")
F
str_view(F)
F<-c('He said"that's amazing!'', "\a\b\c\d'", "\\\\\\\"")
library(tidyverse)
str(F)
str(P)
str(c)
str(data)
str(Mkt_RF)
F<-cchar('He said"that's amazing!'', "\a\b\c\d'", "\\\\\\\"")
F<-c(char('He said"that's amazing!'', "\a\b\c\d'", "\\\\\\\""))
summary(data)
F<-c('He said"that's amazing!'', "\a\b\c\d'", "\\\\\\\"")
df<-c(char('He said"that's amazing!'', "\a\b\c\d'", "\\\\\\\""))
string1<- "This is a string."
read(string1)
read(string1)
a<-'apple'
a
ls(x)
ls()
c(2,4,3)
2:6
seg(2:3, by=0.5)
seq(2:3, by=0.5)
c(seq(2:3, by=0.5))
c(legth(seq(2:3, by=0.5)))
rep(1:2,times=3)
seq(2:3, by=0.5)
seq(2:3, each=3)
sequence(2:3, by=0.5)
sequence(2:3, by=2)
sequence(2:3, by=2)
sequence(2:3, each=3)
sort(c)
sort(c(sequence(inrease)))
sort(c(sequence( ,)))
rev(c)
View(C)
nchar(x)
nchar(16)
nchar(17)
count(17)
count(numeric(17, length=10))
count(numeric(char=17, length=))
couunt(x(17))
count(x(17))
aov
aov(sd)
nrows(df)
ncol(data)
nrow(data)
dim(data)
t(m)
t(betas)
l$county
summary(likelihood)
summary(Rm_post2000)
summary(Rm_stats)
poisson.test()
poissgamexch(Rm_post2000)
poisson(link = "log")
poisson.gamma.mix(Rm)
while(i<5){print(i)}
while(i<5){print(i) i<-i+1}
data1=matrix(nrow=4, ncol=4)
data1=matrix(nrow=4, ncol=4, [1:16])
data1=matrix(data1,nrow=4, ncol=4, 1:16])
data1=matrix(k,nrow=4, ncol=4, 1:16])
data1=as.numeric(matrix(k,nrow=4, ncol=4, 1:16]))
data1=as.numeric.matrix(k,nrow=4, ncol=4, 1:16])
data1=as.numeric.matrix(k,nrow=4, ncol=4, 1:16)
text=readLines("http : / /algo .scu.edu/~sanjivdas / ")
text=readLines("http : / /algo .scu.edu/~sanjivdas / ")
text=readLines("http://algo.scu.edu/~sanjivdas/")
text[1:4]
text=readLines("http://algo.scu.edu/~sanjivdas/")
text=readLines("https://www.blogger.com/blog/posts/709891733129813226?bpli=1&pli=1")
text[1:4]
substr(text[4],24,29)
res=regexpr(("Sanjiv",text[4]))
res
res=regexpr(("Lomesh",text[4]))
res=regexpr(("Lomesh" ,text[4]))
res=regexpr(("Lomesh" ,text[4]))
text=readLines("http://algo.scu.edu/~sanjivdas/")
text=readLines("http://algo.scu.edu/~sanjivdas/")
text=readLines("http://algo.scu.edu/~sanjivdas/")
text=readLines("http://algo.scu.edu/-sanjivdas/")
substr(text[1],24,29)
res=regexpr(("ng=\"en" ,text[1]))
res=regexpr(('"ng=\"en"' ,text[1]))
attr(,"match.length")
attr( ,"match.length")
attr( , useBytes)
substr(text[1],first,Last)
substr(text[1],Scale,Last)
substr(text[1],Scale,Last)
substr(text[1],"Scale",Last)
substr(text[1],"Scale", )
nchar(<!doctype html><html lang=\"en-US\" dir=\"ltr\"><head><base href=\"https://accounts.)
line <- text[1]
str_extract(line, "google")
library(stringr)
str_extract(line, "google")
substr(text[1], "scale", )
pos <- regexpr("google", line)   # starting position of "Sanjiv"#
substr(line, pos, pos + attr(pos, "match.length") - 1)
attr ( google, "match. length" )
attr("google","match.length")
attr("google", "useBytes")
substr( text [1] , res [1] , res [1]+nchar( "Sanjiv")-1)
nchar("google")
res <- regexpr("google", text[4])
res
pos <- regexpr("google", line)
res[1]
res <- regexpr("google", text[4])
res
substr(line, pos, pos + attr(pos,"match.length")-1)
nchar(<!doctype html><html lang=\"en-US\" dir=\"ltr\"><head><base href=\"https://accounts.)
Text=readLines("https://www.facebook.com/lomesh.kumar.5"")
>
Text=readLines("https://www.facebook.com/lomesh.kumar.5")
text=readLines("https://www.facebook.com/lomesh.kumar.5")
text[1:4]
line<-text[1]
nchar(<!doctype html><html lang=\"en-US\" dir=\"ltr\"><head><base href=\"https://accounts.)
str_extract(line,"facebook)
>
pos <- regexpr("facebook", line)   # starting position of "facebook"#
str_extract(line, "facebook")
pos <- regexpr("facebook", line)   # starting position of "facebook"#
substr(line, pos, pos + attr(pos,"match.length")-1)
substr(line, pos, pos + attr(pos,"match.length")-1)
res=regexpr("facebook" ,line)
res
pos <- regexpr("facebook", line)
pos
install.packages(tm)
install.packages("tm")
text=c("Resume Khem Singh", "Ashwani", "Lomesh Kumar Resume_3p")
library(tm)
Ctext=Corpus(VectorSource(text))
Ctext
writeCorpus(text)
writeCorpus(ctext)
writeCorpus(Ctext)
inspect(Ctext)
create_date
create_date creater
ctext[[3]]
tm_map(ctext,tolower)[[3]]
tm_map(Ctext,tolower)[[3]]
inspect(Ctext)
print(as.character(ctext[[1]]))
print(as.character(Ctext[[1]]))
print(1apply(Ctext[1:2], as.character))
print(lapply(Ctext[1:2], as.character))
text=readLines("http: / /algo.scu.edu/~sanjivdas/bio candid.html")
text=readLines("http: / /algo.scu.edu/~sanjivdas/bio candid.html")
text=readLines("http: / /algo.scu.edu/~sanjivdas/bio candid.html")
text=readLines("http://algo.scu.edu/~sanjivdas/biocandid.html")
library(tm)
text=readLines("http://algo.scu.edu/~sanjivdas/biocandid.html")
ctext=Corpus(VectorSource(text))
text=readLines("Ashwani")
text=readLines("Ashwani")
text=readLines("Ashwani.docx")
ctext=Corpus(VectorSource(text))
ctext
ctext [[6]]
tm_map(ctext,removePunctuation)[[69]]
ctext [[69]]
ctext[[1]]
ctext[[10]]
ctext[[5]]
ctext[[2]]
ctext[[1]]
tm_map(ctext,removePunctuation)[[1]]
tm_map(ctext,removePunctuation)[[1]]
View(ctext)
tdm_text=TermDocumentMatrix(ctext,control=list(minWordLength=1))
tdm_text
tdm_text=TermDocumentMatrix(ctext,control=list(minWordLength=1))
clean_text <- iconv(text, from = "UTF-8", to = "ASCII//TRANSLIT", sub = "")
ctext <- Corpus(VectorSource(clean_text))
ctext
ctext[[1]]
tm_map(ctext,removePunctuation)[[1]]
tdm_text=TermDocumentMatrix(ctext,control=list(minWordLength=1))
tdm_text
inspect(tdm_text[1:10,1:5])
findFreqTerms(tdm_text ,lowfreq=7)
text=readLines("Lomesh Kumar Resume.docx")
text=readLines("Lomesh Kumar Resume_3p.docx")
text=readLines("Lomesh Kumar Resume_3p.docx")
clean_text <- iconv(text, from = "UTF-8", to = "ASCII//TRANSLIT", sub = "")
ctext <- Corpus(VectorSource(clean_text))
ctext
tm_map(ctext,removePunctuation)[[1]]
tdm_text=TermDocumentMatrix(ctext,control=list(minWordLength=1))
tdm_text
inspect(tdm_text[1:10,1:5])
library(tm)
text=readLines("Data Science Notes.docx")
clean_text <- iconv(text, from = "UTF-8", to = "ASCII//TRANSLIT", sub = "")
ctext <- Corpus(VectorSource(clean_text))
ctext
tm_map(ctext,removePunctuation)[[1]]
tdm_text=TermDocumentMatrix(ctext,control=list(minWordLength=1))
tdm_text
inspect(tdm_text[1:10,1:5])
findFreqTerms(tdm_text ,lowfreq=7)
library(tm)
tdm_mat = as.matrix(tdm)
tdm_mat = as.matrix(tdm)
tdm_text=TermDocumentMatrix(ctext,control=list(minWordLength=1))
tdm_text
inspect(tdm_text[1:10,1:5])
tdm_mat = as.matrix(tdm)
tdm_mat = as.matrix(tdm_text)
tdm_mat = as.matrix(tdm_text)
print (dim(tdm_mat))
nw = dim(tdm_mat)[1]
nd = dim(tdm_mat)[2]
d = 13
w = "derivatives"
f = tdm_mat[w,d]/sum(tdm_mat[ ,d])
f=tdm_mat[w,d]/sum(tdm_mat[,d])
print(f)
w = "0]0iq"
f=tdm_mat[w,d]/sum(tdm_mat[,d])
print(f)
TF = log(f)
print(TF)
d = 10
f=tdm_mat[w,d]/sum(tdm_mat[,d])
print(f)
d = 1\026?\022b\0056?idz\177^\037\025f\023t<{01$?vgr>jx1/\02494$\022udk2"
w = "\026?\022b\0056?idz\177^\037\025f\023t<{01$?vgr>jx1/\02494$\022udk2"
f=tdm_mat[w,d]/sum(tdm_mat[,d])
print(f)
nw=length(which(tdm_mat[w,]>0))
print(nw)
IDF = nd/nw
print(IDF)
#COMPUTE TF-IDF
F_IDF =TF*IDF
print(TF_IDF)
TF_IDF =TF*IDF
print(TF_IDF)
library(worldcloud)
install.packages("wordcloud")
library(wordcloud)
library(tm)
wordcount=sort(rowSums(tdm), decreasing=TRUE)
tdm=as.matrix(tdm_mat)
wordcount=sort(rowSums(tdm), decreasing=TRUE)
tdm_names=names(wordcount)
wordcloud(tdm_names, wordcount)
warnings()
library(tm)
library(NLP)
text = c("Doc1_is_datavision", "Doc2_is_datatable", "Doc3_is_data",
"Doc4_is_nodata", "Doc5_is_simpler")
print(text)
print(gsub("data" ,"" ,text))
# #Remove all words that contain "data" at the start even if they are longer than data
print(gsub("*data.*","",text))
print(gsub("*.data*","",text))
print(gsub("*.data.*" ,"" ,text))
x = c("234-5678","234 5678","2345678","1234567890","0123456789","abc 234 5678","234 5678 def","xx 2345678","abc1234567890def")
idx = grep("[[:digit:]]{3} [[:digit:]]{4}|[[:digit:]]{3} [[:digit:]]{4}|
[1 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9]",x)
print(idx)
idx = grep("[[:digit:]]{3}-[[:digit:]]{4}|[[:digit:]]{3} [[:digit:]]{4}|
[1 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9]",x)
print(idx)
idx = grep("[[:digit:]]{3}-[[:digit:]]{4}|[[:digit:]]{3}-[[:digit:]]{4}|
[1 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9]",x)
print(idx)
idx = grep("[[:digit:]]{3}-[[:digit:]]{4}|[[:digit:]]{3}_[[:digit:]]{4}|
[1 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9]",x)
print(idx)
idx = grep("[[:digit:]]{3}-[[:digit:]]{4}|[[:digit:]]{3}[[:digit:]]{4}|
[1 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9]",x)
print(idx)
idx = grep("[[:digit:]]{3}-[[:digit:]]{4}|[[:digit:]]{3}[[:digit:]]{4}|[1 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9][0 9]",x)
print(idx)
print(x[idx])
idx = grep("[[:digit :]]{3}-[[:digit :]]{4}|[[:digit :]]{3} [[:digit :]]{4}|[1 9][0 9]{9}",
idx = grep("[[:digit :]]{3}-[[:digit :]]{4}|[[:digit :]]{3} [[:digit :]]{4}|[1 9][0 9]{9}",x)
print(idx)
idx = grep("[[:digit:]]{3}-[[:digit:]]{4}|[[:digit:]]{3} [[:digit:]]{4}|[1 9][0 9]{9}",x)
print(idx)
print(x(idx))
print(x(idx))
print(x[idx])
pattern = "[[:digit:]]{3} [[:digit:]]{4}|[[:digit:]]{3} [[:digit:]]{4}|[1 9][0 9]{9}"
print(regmatches(x, gregexpr(pattern,x)))
str_extract(x,pattern)
x = c("sanjiv das","srdas@scu.edu","SCU","data@science.edu")
print(grep("\\@" ,x))
print(x[grep("\\@" ,x)])
??tm
tdm_text=c(Corpus(Crude))
inspect(crude[1:2])
library(tm)
inspect(crude[1:2])
inspect(Crude[1:2])
data("crude")
View(crude)
inspect(crude[1:2])
inspect(tdm[1:10, 1:5])
tdm <- TermDocumentMatrix(crude,control = list(removePunctuation = TRUE,
stopwords = TRUE))
inspect(tdm[1:10, 1:5])
tdm_mat = as.matrix(tdm)
print (dim(tdm_mat))
nw = dim(tdm_mat)[1]
nd = dim(tdm_mat)[2]
d = 10
w = "13nation"
f=tdm_mat[w,d]/sum(tdm_mat[,d])
print(f)
TF = log(f)
print(TF)
library(tm)
data("crude")
data("crude")
inspect(crude[1:2])
tdm <- TermDocumentMatrix(crude,control = list(minWordLength=1)
inspect(tdm[1:10, 1:5])
tdm <- TermDocumentMatrix(crude,control=list(minWordLength=1))
inspect(tdm[1:10, 1:5])
tdm_mat = as.matrix(tdm)
print (dim(tdm_mat))
nw = dim(tdm_mat)[1]
nd = dim(tdm_mat)[2]
w = "opec"
f=tdm_mat[w,d]/sum(tdm_mat[,d])
print(f)
TF = log(f)
print(TF)
nw=length(which(tdm_mat[w,]>0))
print(nw)
IDF = nd/nw
print(IDF)
TF_IDF =TF*IDF
print(TF_IDF)
findMostFreqTerms(tdm,freq=7)
findFreqTerms(tdm,lowfreq=7)
library(wordcloud)
tdm=as.matrix(tdm)
tdm
wordcount=sort(rowSums((tdm), decreasing=TRUE))
wordcount=sort(rowSums((tdm), decreasing=TRUE)
tdm_names=names(wordcount)
tdm_names=names(wordcount)
wordcount=sort(rowSums((tdm), decreasing=TRUE)
wordcount=sort(rowSums((tdm), decreasing=TRUE),
wordcount=sort(rowSums((tdm),decreasing=TRUE)
wordcount=sort(rowSums(tdm),decreasing=TRUE)
wordcount=sort(rowSums(tdm),decreasing=TRUE)
wordcount=sort(rowSums(tdm),decreasing=TRUE)
tdm_names=names(wordcount)
required package: Rcpp
library(tm)
require(Rcpp)
require(RColorBrewer)
data("crude")
findFreqTerms(tdm,lowfreq=7)
tdm <- TermDocumentMatrix(crude,control = list(minWordLength=1)
li>
tdm <- TermDocumentMatrix(crude,control = list(minWordLength=1)
tdm
tdm
findFreqTerms(tdm,lowfreq=7)
tdm=as.matrix(tdm)
tdm
wordcount=sort(rowSums(tdm),decreasing=TRUE)
wordcount=sort(rowSums(tdm),decreasing=TRUE)
tdm_names=names(wordcount)
library(tm)
require(Rcpp)
require(RColorBrewer)
wordcount=sort(rowSums(tdm),decreasing=TRUE)
tdm_names=names(wordcount)
ctext =Corpus(VectorSource(crude))
ctext
ctext[[69]]
tdm_text = TermDocumentMatrix(ctext , control=list(minWordLength=1))
tdm_text
inspect(tdm_text[1:10,1:5])
tdm_text = TermDocumentMatrix(ctext, control=list(minWordLength=1))
inspect(tdm_text[1:10,1:5])
data("crude")
inspect(tdm_text[1:10,1:5])
library(wordcloud)
tdm=as.matrix(tdm_text)
tdm
inspect(tdm[1:10,1:5])
inspect(tdm[1:10,1:5])
wordcount=sort(rowSums(tdm),decreasing=TRUE)
tdm_names=names(wordcount)
wordcloud(tdm_names,wordcount)
wordcount=sort(rowSums(tdm),decreasing=TRUE)
tdm_names=names(wordcount)
wordcloud(tdm_names,wordcount)
warnings()
wordcloud(tdm_names,wordcount, min.freq = 3)
wordcloud(tdm_names,wordcount, min.freq = 2)
wordcloud(tdm_names,wordcount, min.freq = 2, max.words = 25)
wordcloud(tdm_names,wordcount, min.freq = 2, max.words = 50)
wordcloud(tdm_names,
wordcount,
min.freq = 2,
max.words = 50,
scale = c(4, 0.5),
colors = brewer.pal(8,"Dark2"))
savehistory("~/wordcloud.Rhistory")
